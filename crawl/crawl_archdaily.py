import os
import random
import time
import re
import requests
from multiprocessing import Pool
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from config import candidate_headers

# ========== é…ç½® ==========
SAVE_DIR = 'archdaily_images'
os.makedirs(SAVE_DIR, exist_ok=True)

MAX_SCROLLS = 50  # æœ€å¤šæ»šåŠ¨æ¬¡æ•°
SCROLL_RETRY = 3  # æ»šåŠ¨åˆ°åº•æ£€æµ‹é‡è¯•æ¬¡æ•°
MAX_RETRIES = 5  # ä¸‹è½½å›¾ç‰‡æœ€å¤§é‡è¯•æ¬¡æ•°
BACKOFF_FACTOR = 1.0  # é€€é¿å› å­ï¼ˆç§’ï¼‰ï¼Œå®é™…ç­‰å¾… = BACKOFF_FACTOR * 2^(attempt-1)
NUM_PROCESSES = 8  # å¹¶å‘è¿›ç¨‹æ•°


# ========== è¾…åŠ©å‡½æ•° ==========
def get_random_header():
    return random.choice(candidate_headers)


def scroll_to_bottom(driver):
    last_height = driver.execute_script("return document.body.scrollHeight")
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)
    new_height = driver.execute_script("return document.body.scrollHeight")
    return new_height > last_height


def extract_hd_image_urls(driver):
    imgs = driver.find_elements(By.CSS_SELECTOR, 'div.gridview__item img')
    urls = set()
    for img in imgs:
        srcset = img.get_attribute("srcset")
        if not srcset:
            continue
        matches = re.findall(r'(https://[^\s,]+)\s+2x', srcset)
        if matches:
            urls.add(matches[-1])
    return sorted(urls)


def download_image(args):
    url, idx = args
    ext = '.webp' if 'format=webp' in url else '.jpg'
    filename = os.path.join(SAVE_DIR, f"img_{idx:04d}{ext}")

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = requests.get(url, headers=get_random_header(), timeout=10)
            if resp.status_code == 200:
                with open(filename, 'wb') as f:
                    f.write(resp.content)
                print(f"âœ… [OK] {filename} (å°è¯• {attempt})")
                return
            else:
                raise requests.RequestException(f"çŠ¶æ€ç  {resp.status_code}")
        except Exception as e:
            wait = BACKOFF_FACTOR * (2 ** (attempt - 1))
            print(f"âš ï¸ [é‡è¯• {attempt}/{MAX_RETRIES}] ä¸‹è½½å¤±è´¥ï¼š{url}\n    åŸå› ï¼š{e}\n    {wait:.1f}s åé‡è¯•â€¦")
            time.sleep(wait)
    print(f"âŒ [å¤±è´¥] {url} å·²è¶…è¿‡ {MAX_RETRIES} æ¬¡é‡è¯•ï¼Œè·³è¿‡ã€‚")


def crawl_images():
    chrome_options = Options()
    # chrome_options.add_argument("--headless")  # å¯é€‰ï¼šæ— ç•Œé¢
    chrome_options.add_argument("--disable-gpu")
    driver = webdriver.Chrome(service=Service(), options=chrome_options)
    driver.get("https://www.archdaily.com/search/images")

    all_urls = set()
    scroll_times = 0
    while scroll_times < MAX_SCROLLS:
        new_urls = extract_hd_image_urls(driver)
        before = len(all_urls)
        all_urls.update(new_urls)
        after = len(all_urls)
        print(f"æ»šåŠ¨ç¬¬ {scroll_times + 1} æ¬¡ï¼Œæ–°å¢ {after - before} å¼ å›¾ï¼Œå…±è®¡ {after} å¼ å›¾")

        # ä¼ªåº•æ£€æµ‹ï¼šé‡è¯•æ»šåŠ¨
        success = False
        for retry in range(1, SCROLL_RETRY + 1):
            if scroll_to_bottom(driver):
                success = True
                break
            else:
                print(f"âš ï¸ ä¼ªåº•æ£€æµ‹ï¼Œé‡è¯•æ»šåŠ¨ {retry}/{SCROLL_RETRY}â€¦")
        if not success:
            print("ğŸ“­ é¡µé¢å·²çœŸÂ·åˆ°åº•")
            break

        scroll_times += 1

    driver.quit()
    return sorted(all_urls)


if __name__ == "__main__":
    urls = crawl_images()
    print(f"å¼€å§‹å¤šè¿›ç¨‹ä¸‹è½½ {len(urls)} å¼ å›¾ç‰‡â€¦")
    args = [(url, i) for i, url in enumerate(urls)]
    with Pool(processes=NUM_PROCESSES) as pool:
        pool.map(download_image, args)
